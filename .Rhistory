data(segmentationOriginal)
library(caret)
head(segmantationOriginal)
head(segmentationOriginal)
training<-segmentationOriginal[, Case=="Test"]
training<-segmentationOriginal[, select(Case=="Test")]
training<-subset(segmentationOriginal, Case=="Test")
training<-subset(segmentationOriginal, Case=="Train")
testing<-subset(segmentationOriginal, Case=="Test")
model<-train(Case~., data = segmentationOriginal[,-c("Case")],method="rpart")
model<-train(Case~., data = segmentationOriginal[,-"Case"],method="rpart")
model<-train(Case~., data = segmentationOriginal[,-Case],method="rpart")
model<-train(Case~., data = segmentationOriginal,method="rpart")
Tot<-c(23000, 50000, 57000, NA)
Fiber<-c(10, 10, 8, 8)
Perim<-c(2, NA, NA, 2)
VarIn<-c(NA, 100, 100, 100)
df<- data.frame(TotalIntench2 = Tot, FiberWidthCh1 = Fiber, PerimStatusCh1=Perim, VarIntenCh4=VarIn)
predict(model, df)
model<-train(Class~., data = subset(segmentationOriginal, select = -c("Case")),method="rpart")
model<-train(Class~., data = subset(segmentationOriginal, select = -c(Case)),method="rpart")
Tot<-c(23000, 50000, 57000, NA)
Fiber<-c(10, 10, 8, 8)
Perim<-c(2, NA, NA, 2)
VarIn<-c(NA, 100, 100, 100)
df<- data.frame(TotalIntench2 = Tot, FiberWidthCh1 = Fiber, PerimStatusCh1=Perim, VarIntenCh4=VarIn)
predict(model, df)
names(df)
names(segmentationOriginal)
print(model$finalModel)
plot(model$finalModel)
plot(model$finalModel, uniform=TRUE)
plot(model$finalModel, uniform=TRUE)
text(model$finalModel, all=TRUE)
training<-subset(segmentationOriginal, Case=="Train")
testing<-subset(segmentationOriginal, Case=="Test")
set.seed(125)
model<-train(Class~., data = subset(segmentationOriginal, select = -c(Case)),method="rpart")
set.seed(125)
model<-train(Class~., data = subset(training, select = -c(Case)),method="rpart")
print(model$finalModel)
plot(model$finalModel, uniform=TRUE)
text(model$finalModel, all=TRUE)
library(pgmm)
install.packages("pgmm")
c
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
str(olive)
model<-train(Area~., data = olive,method="rpart")
newdata = as.data.frame(t(colMeans(olive)))
pr<-predict(model, newdata)
pr
unique(olive$Area)
library(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
str(trainSA)
model<-train(chd~age+alcohol+obesity+tobacco+typea+ldl, data=trainSA, method="glm", family="binomial")
model<-train(factor(chd)~age+alcohol+obesity+tobacco+typea+ldl, data=trainSA, method="glm", family="binomial")
set.seed(13234)
model<-train(factor(chd)~age+alcohol+obesity+tobacco+typea+ldl, data=trainSA, method="glm", family="binomial")
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
prtr<-predict(model, training)
prtr<-predict(model, trainSA)
prte<-predict(model, testSA)
missClass(trainSA$chd, prtr)
set.seed(13234)
model<-train(chd~age+alcohol+obesity+tobacco+typea+ldl, data=trainSA, method="glm", family="binomial")
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
prtr<-predict(model, trainSA)
prte<-predict(model, testSA)
missClass(trainSA$chd, prtr)
missClass(testSA$chd, prte)
install.packages("ElemStatLearn")
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
str(vowel.train)
vowel.train$y<-factor(vowel.train$y)
vowel.test$y<-factor(vowel.test$y)
set.seed(33833)
varImp(model)
library(caret)
varImp(model)
set.seed(33833)
model<-train(y~., data=vowel.train, method="rf")
model<-train(y~., data=vowel.train, method="rf")
varImp(model)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
str(vowel.train)
set.seed(33833)
mod1<-train(y~.,data=vowel.train, method="rf")
library(caret)
mod1<-train(y~.,data=vowel.train, method="rf")
mod2<-train(y~., data=vowel.train, method="gbm")
mod2<-train(y~., data=vowel.train, method="gbm")
pr1<-predict(mod1, data=vowel.test)
pr1<-predict(mod1, vowel.test)
pr2<-predict(mod2, vowel.test)
sum(pr1==vowel.test$y)/dim(vowel.test)[1]
pr1
sum(round(pr1)==vowel.test$y)/dim(vowel.test)[1]
pr1<-predict(mod1, vowel.test, type="class")
mod1<-train(y~.,data=vowel.train, method="rf", type="class")
mod2<-train(y~., data=vowel.train, method="gbm", type="class")
pr1<-predict(mod1, vowel.test, type="class")
mod1<-randomForest(y~.,data=vowel.train)
mod2<-gbm(y~.,data=vowel.train)
pr1<-predict(mod1, vowel.test, type="class")
pr2<-predict(mod2, vowel.test, type="class")
sum(pr1==vowel.test$y)/dim(vowel.test)[1]
pr1
sum(round(pr1)==vowel.test$y)/dim(vowel.test)[1]
pr1<-predict(mod1, vowel.test, type="class")
pr2<-predict(mod2, vowel.test, type="class")
names(getModelInfo())
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
mod1<-train(diagnosis~., data=training, method="rf")
mod2<-train(diagnosis~., data=training, method="gbm")
mod3<-train(diagnosis~., data=training, method="lda")
pr1<-predict(mod1, testing)
pr1
sum(pr1==testing$diagnosis)/dim(testing)[1]
pr2<-predict(mod2, testing)
sum(pr2==testing$diagnosis)/dim(testing)[1]
pr3<-predict(mod3, testing)
sum(pr3==testing$diagnosis)/dim(testing)[1]
newframe<-data.frame(pr1, pr2, pr3, testing$diagnosis)
summary(newframe)
newframe<-data.frame(pr1, pr2, pr3, diag=testing$diagnosis)
summary(newframe)
mod<-train(diag~., data=newframe, method="rf")
pr<-predict(mod, testing)
sum(pr==newframe$diag)/dim(newframe)[1]
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
?plot.enet
?lasso
?relaxo
mod<-train(diagnosis~., data=training, method="enet")
str(training)
mod<-train(CompressiveStrength~., data=training, method="enet")
mod<-train(CompressiveStrength~., data=training, method="enet")
?plot.enet
plot.enet(mod)
?enet
mod<-enet(subset(training, select=-c("CompressiveStrength")),training$CompressiveStrength, lambda=0.1)
mod<-enet(subset(training, select=-c(CompressiveStrength)),training$CompressiveStrength, lambda=0.1)
str(training)
str(subset(training, select=-c(CompressiveStrength)))
?as.matrix
mod<-enet(as.matrix(subset(training, select=-c(CompressiveStrength))),training$CompressiveStrength, lambda=0.1)
plot.enet(mod)
?plot.enet
mod<-enet(as.matrix(subset(training, select=-c(CompressiveStrength))),training$CompressiveStrength, lambda=0.1, xvar="penalty")
mod<-enet(as.matrix(subset(training, select=-c(CompressiveStrength))),training$CompressiveStrength, lambda=0.1)
plot.enet(mod,xvar="penalty")
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
install.packages("forecast")
?bats
dat = read.csv("C:/Users/Basov_il/Documents/GitHub/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
mod<-bats(training)
library(forecast)
bats
?bats
mod<-bats(training)
str(training)
mod<-bats(tstrain)
plot(forecast(mod))
lines(testing$visitsTumblr)
summary(mod)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
install.packages("e1071")
install.packages("e1071")
library(e1071)
?svm
set.seed(325)
str(training)
mod<-train(CompressiveStrength~., data=training, method="svm")
?svm
library("tm")
set.seed(123)
selectRandomData<-function(vec, prop){
selects<-rbinom(length(vec),1,prop)==1
vec[selects]
}
readDocuments<-function(path, prop){
con <- file(path, "r")
docs<-readLines(con)
close(con)
selectRandomData(docs, prop)
}
setwd("c:/Users/Basov_il/Documents/GitHub/Capstone/")
twits<-readDocuments("final/en_US/en_US.twitter.txt", 0.0001)
news<-readDocuments("final/en_US/en_US.news.txt", 0.0001)
blogs<-readDocuments("final/en_US/en_US.blogs.txt", 0.0001)
data<-c(twits, blogs, news)
```
```{r}
remove(twits)
remove(blogs)
remove(news)
corps <- VCorpus(VectorSource(data))
remove(data)
corps <- tm_map(corps, removePunctuation)
corps <- tm_map(corps, removeNumbers)
corps <- tm_map(corps, removeWords, badwords)
badwords <- readLines("badwords.txt")
corps <- tm_map(corps, removeWords, badwords)
corps <- tm_map(corps, stripWhitespace)
tdm<-TermDocumentMatrix(corps)
inspect(tdm[1:50,1:10])
tdmFreq<-removeSparseTerms(tdm, 0.98)
dim(tdmFreq)
inspect(tdmFreq)
mat<-as.matrix(tdmFreq)
library(reshape2)
mat <- melt(mat, value.name = "count")
library(ggplot2)
ggplot(mat, aes(x = Docs, y = Terms, fill = log10(count))) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
ylab("")
ggplot(mat, aes(x = Docs, y = Terms, fill = log10(count))) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
ylab("")
df<-NULL
terms <- tdmFreq$dimnames$Terms
for (el in terms){
assocs<-findAssocs(tdmFreq, el, 0)
res<-rep(0, length(terms))
names(res)<-terms
l<-list(x<-res)
names(l)<-el
for (term in terms){
values<-assocs[[el]]
if (term %in% names(values)){
l[[el]][term]<-values[term]
}
}
if (is.null(df)){
df<-data.frame(l)
}else {
df[el]<-l
}
}
mat<-as.matrix(df)
library(reshape2)
mat <- melt(mat, value.name = "count")
ggplot(mat, aes(x = Var1, y = Var2, fill = log10(count))) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
ylab("")
install.packages("RWeka")
library(RWeka)
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tri_tdm <- TermDocumentMatrix(a, control = list(tokenize = TrigramTokenizer))
tri_tdm <- TermDocumentMatrix(corps, control = list(tokenize = TrigramTokenizer))
inspect(tri_tdm[1:50,1:10])
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.98)
inspect(tri_tdmFreq[1:50,1:10])
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.99)
inspect(tri_tdmFreq[1:50,1:10])
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.99999)
inspect(tri_tdmFreq[1:50,1:10])
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.9999)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.999)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.99)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.995)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.997)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.996)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.9995)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.9991)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.999)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.996)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.997)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.9965)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.9966)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.9967)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.9968)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.9969)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.99699)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.99698)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.99697)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.99696)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.99695)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.99694)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.99693)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.99692)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.997)
dim(tri_tdmFreq)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.996)
dim(tri_tdmFreq)
inspect(tri_tdmFreq[,1:10])
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bi_tdm <- TermDocumentMatrix(corps, control = list(tokenize = TrigramTokenizer))
dim(bi_tdmFreq)
bi_tdmFreq<-removeSparseTerms(bi_tdm, 0.996)
dim(bi_tdmFreq)
inspect(bi_tdmFreq[,1:10])
bi_tdm <- TermDocumentMatrix(corps, control = list(tokenize = BigramTokenizer))
bi_tdmFreq<-removeSparseTerms(bi_tdm, 0.996)
dim(bi_tdmFreq)
bi_tdmFreq<-removeSparseTerms(bi_tdm, 0.98)
dim(bi_tdmFreq)
bi_tdmFreq<-removeSparseTerms(bi_tdm, 0.99)
dim(bi_tdmFreq)
inspect(bi_tdmFreq[,1:10])
readDocuments<-function(path, prop){
con <- file(path, "r")
docs<-readLines(con)
close(con)
print("Number of lines: "+length(docs))
print("Number of words: "+sapply(docs,function(x) length(unlist(strsplit(x,split=" ")))))
selectRandomData(docs, prop)
}
news<-readDocuments("final/en_US/en_US.news.txt", 0.0001)
length(newa)
length(news)
len(news)
readDocuments<-function(path, prop){
con <- file(path, "r")
docs<-readLines(con)
close(con)
cat("Total number of lines: ",length(docs))
cat("Total number of words: ",sapply(docs,function(x) length(unlist(strsplit(x,split=" ")))))
selectRandomData(docs, prop)
}
news<-readDocuments("final/en_US/en_US.news.txt", 0.0001)
readDocuments<-function(path, prop){
con <- file(path, "r")
docs<-readLines(con)
close(con)
cat("Total number of lines: ",length(docs))
cat("Total number of words: ",sum(sapply(docs,function(x) length(unlist(strsplit(x,split=" "))))))
selectRandomData(docs, prop)
}
news<-readDocuments("final/en_US/en_US.news.txt", 0.0001)
readDocuments<-function(path, prop){
con <- file(path, "r")
docs<-readLines(con)
close(con)
cat("Total number of lines: ",length(docs),"\n")
cat("Total number of words: ",sum(sapply(docs,function(x) length(unlist(strsplit(x,split=" "))))),"\n")
selectRandomData(docs, prop)
}
news<-readDocuments("final/en_US/en_US.news.txt", 0.0001)
twits<-readDocuments("final/en_US/en_US.twitter.txt", 0.0001)
set.seed(123)
selectRandomData<-function(vec, prop){
selects<-rbinom(length(vec),1,prop)==1
cat("Selected number of lines: ",sum(selects),"\n")
vec[selects]
}
readDocuments<-function(path, prop){
con <- file(path, "r")
docs<-readLines(con)
close(con)
cat("Total number of lines: ",length(docs),"\n")
selectRandomData(docs, prop)
}
news<-readDocuments("final/en_US/en_US.news.txt", 0.0001)
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
uni_tdm <- TermDocumentMatrix(corps, control = list(tokenize = UnigramTokenizer))
inspect(uni_tdmFreq[1:10,1:10])
inspect(uni_tdm[1:10,1:10])
inspect(uni_tdm[,1:10])
dim(uni_tdm)
inspect(uni_tdm[1:10,1:10])
uni_tdmFreq<-removeSparseTerms(bi_tdm, 0.99)
dim(uni_tdmFreq)
dim(uni_tdm)
inspect(uni_tdm[1:10,1:10])
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bi_tdm <- TermDocumentMatrix(corps, control = list(tokenize = BigramTokenizer))
dim(bi_tdm)
inspect(bi_tdm[1:10,1:10])
bi_tdmFreq<-removeSparseTerms(bi_tdm, 0.99)
dim(bi_tdmFreq)
inspect(bi_tdmFreq[,1:10])
library(reshape2)
library(ggplot2)
freq_m<-as.matrix(uni_tdmFreq)
freq_m <- melt(freq_m, value.name = "Count")
ggplot(freq_m, aes(x = Docs, y = Terms, fill = log10(count))) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#00FF00" , low="#FFFFFF")+
ylab("")
freq_m<-as.matrix(uni_tdmFreq)
freq_m <- melt(freq_m, value.name = "Count")
ggplot(freq_m, aes(x = Docs, y = Terms, fill = log10(Count))) +
scale_fill_gradient(high="#00FF00" , low="#FFFFFF")+
ylab("")
geom_tile(colour = "white") +
df<-NULL
freq_m<-as.matrix(uni_tdmFreq)
freq_m <- melt(freq_m, value.name = "Count")
ggplot(freq_m, aes(x = Docs, y = Terms, fill = log10(Count))) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#00FF00" , low="#FFFFFF")+
ylab("")
ggplot(freq_m, aes(x = Docs, y = Terms, fill = log10(Count))) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#0000FF" , low="#FFFFFF")+
ylab("")
ggplot(freq_m, aes(x = Docs, y = Terms, fill = log10(Count))) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
ylab("")
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
uni_tdm <- TermDocumentMatrix(corps, control = list(tokenize = UnigramTokenizer))
uni_tdmFreq<-removeSparseTerms(uni_tdm, 0.99)
dim(uni_tdmFreq)
freq_m<-as.matrix(uni_tdmFreq)
freq_m <- melt(freq_m, value.name = "Count")
ggplot(freq_m, aes(x = Docs, y = Terms, fill = log10(Count))) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
ylab("")
freq_m<-as.matrix(bi_tdmFreq)
freq_m <- melt(freq_m, value.name = "Count")
ggplot(freq_m, aes(x = Docs, y = Terms, fill = log10(Count))) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
ylab("")
freq_m<-as.matrix(tri_tdmFreq)
freq_m <- melt(freq_m, value.name = "Count")
ggplot(freq_m, aes(x = Docs, y = Terms, fill = log10(Count))) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
ylab("")
dim(tri_tdm)
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.996)
dim(tri_tdmFreq)
freq_m<-as.matrix(tri_tdmFreq)
freq_m <- melt(freq_m, value.name = "Count")
ggplot(freq_m, aes(x = Docs, y = Terms, fill = log10(Count))) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
ylab("")
head(uni_tdmFreq)
source('~/.active-rstudio-document', echo=TRUE)
typeof(uni_tdmFreq)
dim(uni_tdmFreq)
uni_tdmFreq
uni_tdmFreq[[1]]
uni_tdmFreq$Terms
uni_tdmFreq$Terms
uni_tdmFreq[[1]]$Terms
uni_tdmFreq$dimnames$Terms
UnigramTokenizer <- NGramTokenizer(data, Weka_control(min = 1, max = 1))
uni_tdm <- TermDocumentMatrix(corps, control = list(tokenize = UnigramTokenizer))
