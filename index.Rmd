---
title: "Capstone Project: Predicting the next word"
output: html_document
---
Introduction
------------------------
Our goal in this whole project is to use text chunks, obtained from news, twitter and blogs for predictiong next word in user input.
The data is from a corpus called HC Corpora (www.corpora.heliohost.org). Description for corpus can be found here http://www.corpora.heliohost.org/aboutcorpus.html.
In the current document we make some exploratory analysis of the data in order to understand it's structure and come up with appropriate strategies for predicting words.

Obtaining data
----------------------------------

At this point we already know that the whole data is too big to be placed in one computer's memory, so let's define functions for reading in documents, showing number of lines and forming united list from random portions of predefined size.

```{r}
set.seed(123)
selectRandomData<-function(vec, prop){
    selects<-rbinom(length(vec),1,prop)==1
    cat("Selected number of lines: ",sum(selects),"\n")
    vec[selects]
}

readDocuments<-function(path, prop){
    con <- file(path, "r") 
    docs<-readLines(con)
    close(con)
    cat("Total number of lines: ",length(docs),"\n")
    selectRandomData(docs, prop)
}

twits<-readDocuments("final/en_US/en_US.twitter.txt", 0.1)
news<-readDocuments("final/en_US/en_US.news.txt", 0.1)
blogs<-readDocuments("final/en_US/en_US.blogs.txt", 0.1)

data<-c(twits, blogs, news)

```

At this point we have list of documents, consisting of 10% from each source.

Cleaning data
------------------------------

Now let's make a corpus from our collection of documents and clean it from punctuation and numbers.

```{r}
library("tm")
corps <- VCorpus(VectorSource(data))

corps <- tm_map(corps, removePunctuation)
corps <- tm_map(corps, removeNumbers)
```

We also want to delete profanity words (obtained from http://www.freewebheaders.com/full-list-of-bad-words-banned-by-google/) and remove whitespaces that are left after transformations.

```{r}
badwords <- readLines("badwords.txt")
corps <- tm_map(corps, removeWords, badwords)
corps <- tm_map(corps, stripWhitespace)
```

Tokenization
------------------------------
At this point we are ready for splitting our documents to tokens. We will use 1-word, 2-word and 3-word chunks to explore data and look at frequencies.

```{r}
library(RWeka)
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
uni_tdm <- TermDocumentMatrix(corps, control = list(tokenize = UnigramTokenizer))

BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bi_tdm <- TermDocumentMatrix(corps, control = list(tokenize = BigramTokenizer))

TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tri_tdm <- TermDocumentMatrix(corps, control = list(tokenize = TrigramTokenizer))

FourgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
four_tdm <- TermDocumentMatrix(corps, control = list(tokenize = FourgramTokenizer))
```

Exploratory analysis
--------------------------------------
We can look at histograms, showing the frequencies of different word chunks.
```{r}
library(slam)
uni_freq<-row_sums(uni_tdm)
uni_freq<-sort(uni_freq, decreasing=T)
par(mar=c(10,4,2,2))
barplot(uni_freq[1:50], las=2, main="Most frequent unigrams")

bi_freq<-row_sums(bi_tdm)
bi_freq<-sort(bi_freq, decreasing=T)
barplot(bi_freq[1:50], las=2, main="Most frequent bigrams")

tri_freq<-row_sums(tri_tdm)
tri_freq<-sort(tri_freq, decreasing=T)
barplot(tri_freq[1:50], las=2, main="Most frequent trigrams")

four_freq<-row_sums(four_tdm)
four_freq<-sort(four_freq, decreasing=T)
barplot(four_freq[1:50], las=2, main="Most frequent fourgrams")

```

As we see most common chunks mostly consist of language stop words which is expected.

Now let's explore how many tokens cover which percentage of the whole number of words in collections.

```{r}
explore_terms<-function(freq, step){
  total_sum<-sum(freq)
  freq<-sort(freq, decreasing=T)
  step<-floor(total_sum*step)
  count<-step
  cover<-c()
  counts<-c()
  while(count<=total_sum){
    prop<-sum(freq[1:count], na.rm=T)/total_sum
    cover<-c(cover,prop)
    counts<-c(counts,count)
    if(prop==1) break
    count<-count+step
  }
  plot(counts, cover)
}
length(uni_freq)
explore_terms(uni_freq,0.001)
```

As we see on this plot 90% of words are covered by just 15000 unigram tokens.
```{r}
length(bi_freq)
explore_terms(bi_freq,0.01)
```

At this plot it is seen that 80% covearge occurs at about 700000 bigram tokens.
```{r}
length(tri_freq)
explore_terms(tri_freq,0.01)
```

Here 80% coverage is maintained by approximately 3000000 trigram tokens.

```{r}
length(four_freq)
explore_terms(four_freq,0.01)
```

Here 80% of words are covered by approximately 4000000 fourgram tokens.

Conclusion
-------------------------------
As word prediction by full sentence can be simplifed (by Markov approximation) to prediction by n-grams, we will use our fourgrams, trigrams, bigrams and unigrams (according to backoff method) to predict the next word in user input.
It's quite reasonable not to store all the tokens in memory for prediction purposes because good coverage can be reached with less amount of them in order to achieve memory and speed efficiency (good tradeoff is needed here). 
