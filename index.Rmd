---
title: "Predicting the next word"
output: html_document
---
Introduction
------------------------
Our goal in this whole project is to use text chunks, obtained from news, twitter and blogs for predictiong next word in user input.
The data is from a corpus called HC Corpora (www.corpora.heliohost.org). Description for corpus can be found here http://www.corpora.heliohost.org/aboutcorpus.html.
In the current document we make some exploratory analysis of the data in order to understand it's structure and come up with appropriate strategies for predicting words.

Obtaining data
----------------------------------

At this point we already know that the whole data is too big to be placed in one computer's memory, so let's define functions for reading in documents, showing number of lines and forming united list from random portions of predefined size.

```{r}
set.seed(123)
selectRandomData<-function(vec, prop){
    selects<-rbinom(length(vec),1,prop)==1
    cat("Selected number of lines: ",sum(selects),"\n")
    vec[selects]
}

readDocuments<-function(path, prop){
    con <- file(path, "r") 
    docs<-readLines(con)
    close(con)
    cat("Total number of lines: ",length(docs),"\n")
    selectRandomData(docs, prop)
}

setwd("c:/Users/Basov_il/Documents/GitHub/Capstone/")

twits<-readDocuments("final/en_US/en_US.twitter.txt", 0.0001)
news<-readDocuments("final/en_US/en_US.news.txt", 0.0001)
blogs<-readDocuments("final/en_US/en_US.blogs.txt", 0.0001)

data<-c(twits, blogs, news)

```

Cleaning data
------------------------------

Now let's make a corpus from our collection of documents and clean it from punctuation and numbers.

```{r}
library("tm")
corps <- VCorpus(VectorSource(data))

corps <- tm_map(corps, removePunctuation)
corps <- tm_map(corps, removeNumbers)
```

We also want to delete profanity words (obtained from http://www.freewebheaders.com/full-list-of-bad-words-banned-by-google/) and remove whitespaces that are left after transformations.

```{r}
badwords <- readLines("badwords.txt")
corps <- tm_map(corps, removeWords, badwords)
corps <- tm_map(corps, stripWhitespace)
```

Tokenization
------------------------------
At this point we are ready for splitting our documents to tokens. We will use 1-word, 2-word and 3-word chunks to explore data and look at frequencies.

```{r}
library(RWeka)
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
uni_tdm <- TermDocumentMatrix(corps, control = list(tokenize = UnigramTokenizer))

BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bi_tdm <- TermDocumentMatrix(corps, control = list(tokenize = BigramTokenizer))

TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tri_tdm <- TermDocumentMatrix(corps, control = list(tokenize = TrigramTokenizer))
```

Exploratory analysis
--------------------------------------
First, let's inspect matrices that we've got at the previous stage.

```{r}
dim(uni_tdm)
inspect(uni_tdm[1:10,1:10])
```
We see that there are more than 2000 unique terms and at the same time matrix is very sparse. We can try improve it by removing very sparse terms.
```{r}
uni_tdmFreq<-removeSparseTerms(uni_tdm, 0.99)
dim(uni_tdmFreq)
```

Only 64 left. Let's stay with them and do the same operations with two other matrices with appropriate filtering.

```{r}
dim(bi_tdm)
inspect(bi_tdm[1:10,1:10])
bi_tdmFreq<-removeSparseTerms(bi_tdm, 0.99)
dim(bi_tdmFreq)

dim(tri_tdm)
inspect(tri_tdmFreq[1:10,1:10])
tri_tdmFreq<-removeSparseTerms(tri_tdm, 0.996)
dim(tri_tdmFreq)
```

We can look at histograms, showing the frequencies of different word chunks.
```{r}
uni_Freq<-apply(uni_tdmFreq,1,sum)
uni_Freq<-sort(uni_Freq, decreasing=T)
barplot(uni_Freq[1:20], las=2)

bi_Freq<-apply(bi_tdmFreq,1,sum)
bi_Freq<-sort(bi_Freq, decreasing=T)
barplot(bi_Freq[1:20], las=2)

tri_Freq<-apply(tri_tdmFreq,1,sum)
tri_Freq<-sort(tri_Freq, decreasing=T)
barplot(tri_Freq[1:20], las=2)
```

Now let's look at the distribution of word chunks among documents (white for one occurence in document, more red for more occurencies).

```{r}
library(reshape2)
library(ggplot2)

freq_m<-as.matrix(uni_tdmFreq)
freq_m <- melt(freq_m, value.name = "Count")
ggplot(freq_m, aes(x = Docs, y = Terms, fill = log10(Count))) +
     geom_tile(colour = "white") +
     scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
     ylab("") 

freq_m<-as.matrix(bi_tdmFreq)
freq_m <- melt(freq_m, value.name = "Count")
ggplot(freq_m, aes(x = Docs, y = Terms, fill = log10(Count))) +
     geom_tile(colour = "white") +
     scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
     ylab("") 

freq_m<-as.matrix(tri_tdmFreq)
freq_m <- melt(freq_m, value.name = "Count")
ggplot(freq_m, aes(x = Docs, y = Terms, fill = log10(Count))) +
     geom_tile(colour = "white") +
     scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
     ylab("") 
     

```
